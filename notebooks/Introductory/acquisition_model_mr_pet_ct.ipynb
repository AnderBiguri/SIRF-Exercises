{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models for MR, PET and CT\n",
    "This demonstration shows how to set-up and use SIRF acquisition models for different modalities.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch\n",
    "First version: 22nd of April 2021  \n",
    "\n",
    "CCP PETMR Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2015 - 2017 Rutherford Appleton Laboratory STFC.  \n",
    "Copyright 2015 - 2019 University College London.   \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import MR, PET and CT functionality\n",
    "import sirf.Gadgetron as mr\n",
    "import sirf.STIR as pet\n",
    "import cil.framework as ct\n",
    "\n",
    "from sirf.Utilities import examples_data_path\n",
    "#from ccpi.astra.operators import AstraProjectorSimple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "# They have (minimal) documentation using Python docstrings such that you \n",
    "# can do for instance \"help(subplot_)\"\n",
    "#\n",
    "\n",
    "def subplot_(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    #idim = templ_im.dimensions() # TODO\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb. We will use a FDG image for PET. MR usually provides qualitative images with an image contrast proportional to difference in T1, T2 or T2* depending on the sequence parameters. Nevertheless, we will make our life easy, by directly using the T1 map provided by the brainweb for MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = FDG_arr.shape[0]//2\n",
    "subplot_([1,3,1], FDG_arr[slice_show, 100:-100, 100:-100], 'FDG', cmap=\"hot\")\n",
    "subplot_([1,3,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")\n",
    "subplot_([1,3,3], uMap_arr[slice_show, 100:-100, 100:-100], 'uMap', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SIRF an `AcquisitionModel` basically contains everything we need to know in order to describe what happens when we go from the imaged object to the acquired raw data (`AcquisitionData`) and then to the reconstructed image (`ImageData`). What we actually need to know depends strongly on the modality we are looking at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of modality specific information:  \n",
    "\n",
    "  * __PET__: scanner geometry, attenuation map, detector efficiency,...  \n",
    "  * __CT__: scanner geometry  \n",
    "  * __MR__: k-space sampling pattern, coil sensitivity information,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then there is information which is independent of the modality such as field-of-view or image resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For __PET__ and __MR__ a lot of this information is already in the raw data. Because it would be quite a lot of work to enter all the necessary information by hand and then checking it is consistent, we create `AcquisitionModel` objects from `AcquisitionData` objects. The `AcquisitionData` only serves as a template and both its actual image and raw data content can be (and in this exercise will be) replaced. For __CT__ we will create an acquisition model from scratch, i.e. we will define the scanner geometry, image dimensions and image voxels sizes and so by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's get started with the __MR__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MR we basically need the following:\n",
    "\n",
    "  1. create an MR `AcquisitionData` object from a raw data file\n",
    "  2. calculate the coil sensitivity maps (csm, for more information on that please see the notebook MR/c_coil_combindation.ipynb). These csm are a derived class from MR `ImageData` and therefore can also be used as a template for the image data we want to reconstruct\n",
    "  3. then we will set up the MR `AcquisitionModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(examples_data_path('MR') + '/grappa2_1rep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)\n",
    "\n",
    "im_mr = csm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create AcquisitionModel\n",
    "acq_mod_mr = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "\n",
    "# Supply csm to the acquisition model \n",
    "acq_mod_mr.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply acquisition models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got an MR image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mr = crop_and_fill(im_mr, T1_arr)\n",
    "plt.figure();\n",
    "subplot_([1,1,1], numpy.abs(im_mr.as_array())[im_mr.dimensions()[0]//2, :, :], 'MR', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Use the 'ct' prefix for all CIL-based SIRF functions.\n",
    "# This is done here to explicitly differentiate between SIRF ct functions and \n",
    "# anything else.\n",
    "import cil.framework as ct #import ImageGeometry, AcquisitionGeometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a template CT acquisition geometry\n",
    "N = 120\n",
    "angles = numpy.linspace(0, 360, 50, True, dtype=numpy.float32)\n",
    "offset = 0.4\n",
    "channels = 1\n",
    "ag = ct.AcquisitionGeometry.create_Cone3D((offset,-100, 0), (offset,100,0))\n",
    "ag.set_panel((N,N-2))\n",
    "ag.set_channels(channels)\n",
    "ag.set_angles(angles, angle_unit=ct.AcquisitionGeometry.DEGREE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a template CT image object\n",
    "ig = ag.get_ImageGeometry()\n",
    "im_ct = ig.allocate(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got an CT image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_ct = crop_and_fill(im_ct, uMap_arr)\n",
    "\n",
    "plt.figure();\n",
    "subplot_([1,1,1], im_ct.as_array()[im_ct.as_array().shape[0]//2, :, :], 'CT', cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Use the 'pet' prefix for all STIR-based SIRF functions.\n",
    "# This is done here to explicitly differentiate between SIRF pet functions and \n",
    "# anything else.\n",
    "import sirf.STIR as pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need a template sinogram\n",
    "templ_sino = pet.AcquisitionData(examples_data_path('PET') + \"/thorax_single_slice/template_sinogram.hs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a template PET image object\n",
    "im_pet = pet.ImageData(templ_sino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got an PET image object and can fill it with the brainweb data. The dimensions won't fit, but we will simply crop the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pet = crop_and_fill(im_pet, FDG_arr)\n",
    "\n",
    "plt.figure();\n",
    "subplot_([1,1,1], im_pet.as_array()[im_pet.dimensions()[0]//2, :, :], 'PET', cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create acquisition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create an acquisition model for MR\n",
    "\n",
    "# First we need the coil sensitivity information\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)\n",
    "\n",
    "# create MR acquisition model\n",
    "acq_mod_mr = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "\n",
    "# to supply coil info to the acquisition model we use the dedicated method\n",
    "acq_mod_mr.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create an acquisition model for PET\n",
    "\n",
    "# First we need a uMap\n",
    "uMap = crop_and_fill(im_pet, uMap_arr)\n",
    "#uMap = crop_and_fill(im_pet, numpy.ones((300,300,300)))\n",
    "\n",
    "# create PET acquisition model\n",
    "acq_mod_pet = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_mod_pet.set_num_tangential_LORs(5)\n",
    "\n",
    "if False:\n",
    "    # Set up sensitivity due to attenuation\n",
    "    asm_attn = pet.AcquisitionSensitivityModel(uMap, acq_mod_pet)\n",
    "    asm_attn.set_up(templ_sino)\n",
    "    bin_eff = pet.AcquisitionData(templ_sino)\n",
    "    bin_eff.fill(1.0)\n",
    "\n",
    "    # Apply attenuation\n",
    "    asm_attn.unnormalise(bin_eff)\n",
    "    asm_attn = pet.AcquisitionSensitivityModel(bin_eff)\n",
    "\n",
    "    acq_mod_pet.set_acquisition_sensitivity(asm_attn)\n",
    "\n",
    "acq_mod_pet.set_up(templ_sino, uMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create an acquisition model for CT\n",
    "\n",
    "#acq_mod_ct = AstraProjectorSimple(ig, ag, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply acquisition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Now we have got all the acquisition models and can apply the forward model for\n",
    "\n",
    "# PET\n",
    "raw_pet = acq_mod_pet.forward(im_pet)\n",
    "\n",
    "# MR\n",
    "raw_mr = acq_mod_mr.forward(im_mr)\n",
    "\n",
    "# CT\n",
    "#raw_ct = acq_mod_ct.forward(im_ct)\n",
    "\n",
    "# and apply the backward model\n",
    "\n",
    "# PET\n",
    "rec_pet = acq_mod_pet.backward(raw_pet)\n",
    "\n",
    "# MR\n",
    "rec_mr = acq_mod_mr.backward(raw_mr)\n",
    "\n",
    "# CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_dat_ct.as_array().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec_pet.as_array().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "# Raw data\n",
    "subplot_([2,3,1], numpy.log(numpy.abs(raw_mr.as_array()[:, raw_mr.dimensions()[1]//2, :])), 'MR raw', cmap=\"viridis\")\n",
    "subplot_([2,3,2], raw_pet.as_array()[0, raw_pet.dimensions()[1]//2, :, :], 'PET raw', cmap=\"viridis\")\n",
    "\n",
    "# Rec data\n",
    "subplot_([2,3,4], numpy.abs(rec_mr.as_array()[rec_mr.dimensions()[0]//2, :, :]), 'MR rec', cmap=\"Greys_r\")\n",
    "subplot_([2,3,5], rec_pet.as_array()[rec_pet.dimensions()[0]//2, :, :], 'PET rec', cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
