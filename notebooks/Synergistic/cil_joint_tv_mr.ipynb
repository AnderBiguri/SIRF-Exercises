{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint TV for multi-contrast MR\n",
    "This demonstration shows how to do a synergistic reconstruction of two MR images with different contrast. Both MR images show the same underlying anatomy but of course with different contrast. In order to make use of this similarity a joint total variation (TV) operator is used as a regularisation in an iterative image reconstruction approach. \n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch, Evangelos Papoutsellis, Edoardo Pasca\n",
    "First version: 16th of June 2021  \n",
    "\n",
    "CCP PETMR Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 Rutherford Appleton Laboratory STFC.    \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "# Import MR functionality\n",
    "import sirf.Gadgetron as mr\n",
    "\n",
    "# Import CIL functionality\n",
    "from sirf.Utilities import examples_data_path\n",
    "from cil.framework import  AcquisitionGeometry, BlockDataContainer, BlockGeometry, ImageGeometry\n",
    "from cil.optimisation.functions import Function, OperatorCompositionFunction, SmoothMixedL21Norm, L1Norm, L2NormSquared, BlockFunction, MixedL21Norm, IndicatorBox, TotalVariation, LeastSquares, ZeroFunction\n",
    "from cil.optimisation.operators import GradientOperator, BlockOperator, ZeroOperator, CompositionOperator, LinearOperator, FiniteDifferenceOperator\n",
    "from cil.optimisation.algorithms import PDHG, FISTA, GD\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint TV reconstruction of two MR images\n",
    "\n",
    "Assume we want to reconstruct two MR images $u$ and $v$ and utilse the similarity between both images using a joint TV ($JTV$) operator we can formulate the reconstruction problem as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(u^{*}, v^{*}) \\in \\underset{u,v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} +  \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* $JTV_{\\eta, \\lambda}(u_{1}, u_{2}) = \\sum \\sqrt{ \\lambda|\\nabla u_{1}|^{2} + (1-\\lambda)|\\nabla u_{2}|^{2} + \\eta^{2}}$\n",
    "* $A_{1}$, $A_{2}$: __MR__ `AcquisitionModel`\n",
    "* $g_{1}$, $g_{2}$ __MR__ `AcquisitionData`\n",
    "\n",
    "\n",
    "### Solving this problem \n",
    "\n",
    "In order to solve the above minimization problem, we will use an alternating minimisation approach, where one variable is fixed and we solve wrt to the other variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u^{k+1} & = \\underset{u}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} + \\alpha_{1}\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v^{k}) \\quad \\text{subproblem 1}\\\\\n",
    "v^{k+1} & = \\underset{v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha_{2}\\,\\mathrm{JTV}_{\\eta, 1-\\lambda}(u^{k+1}, v) \\quad \\text{subproblem 2}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We are going to use a gradient descent approach to solve each of these subproblems alternatingly.\n",
    "\n",
    "The *regularisation parameter* `alpha` should be different for each subproblem. But not to worry at this stage. Maybe we should use $\\alpha_{1}, \\alpha_{2}$ in front of the two JTVs and a $\\lambda$, $1-\\lambda$ for the first JTV and $1-\\lambda$, $\\lambda$, for the second JTV with $0<\\lambda<1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on several other notebooks and hence certain steps will be carried out with minimal documentation. If you want more explainations, then we would like to ask you to refer to the corresponding notebooks which are mentioned in the following list. The steps we are going to carry out are\n",
    "\n",
    "  - (A) Get a T1 and T2 map from brainweb which we are going to use as ground truth $u_{gt}$ and $v_{gt}$ for our reconstruction (further information: `introduction` notebook)\n",
    "  \n",
    "  - (B) Create __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ (further information: `acquisition_model_mr_pet_ct` notebook)\n",
    "  \n",
    "  - (C) Set up the joint TV reconstruction problem\n",
    "  \n",
    "  - (D) Solve the joint TV reconstruction problem (further information on gradient descent: `gradient_descent_mr_pet_ct` notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_arr  = vol['T2']\n",
    "T1_arr   = vol['T1']\n",
    "\n",
    "# Normalise image data\n",
    "T2_arr /= numpy.max(T2_arr)\n",
    "T1_arr /= numpy.max(T1_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = T2_arr.shape[0]//2\n",
    "plot_2d_image([1,2,1], T2_arr[slice_show, 100:-100, 100:-100], 'T2', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Simulate undersampled MR AcquisitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(examples_data_path('MR') + '/simulated_MR_2D_cartesian_Grappa2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "csm_arr = numpy.abs(csm.as_array())\n",
    "\n",
    "plot_2d_image([1,2,1], csm_arr[0, 0, :, :], 'Coil 0', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], csm_arr[2, 0, :, :], 'Coil 2', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use these coilmaps to simulate our __MR__ raw data. These coils maps were obtained from a Shepp-Logan-phantom which unfortunately has got these two ellipses in the centre with very little intensity. Therefore, the coil maps cannot be estimated there and contain mainly noise. Keep that in mind, we will compe back to this observation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create the two __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two MR acquisition models\n",
    "A1 = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "A1.set_coil_sensitivity_maps(csm)\n",
    "\n",
    "A2 = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "A2.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "u_gt = crop_and_fill(im_mr, T1_arr)\n",
    "g1 = A1.forward(u_gt)\n",
    "\n",
    "v_gt = crop_and_fill(im_mr, T2_arr)\n",
    "g2 = A2.forward(v_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check we are going to apply the backward/adjoint operation to do a simply image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "u_simple = A1.backward(g1)\n",
    "v_simple = A2.backward(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "plot_2d_image([1,2,1], numpy.abs(u_simple.as_array())[0, 70:180, 70:180], '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], numpy.abs(v_simple.as_array())[0, 70:180, 70:180], '$v_{simple}$', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look quite poor compared to the ground truth input images, because they are reconstructed from an undersampled k-space. In addition, you can see a strange \"structure\" going through the centre of the brain. This has something to do with the coil maps. As mentioned above, our coil maps have these two \"holes\" in the centre and this creates this artefacts. Nevertheless, this is not going to be a problem for our reconstruction as we will see later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Set up the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used mainly __SIRF__ functionality, now we are going to use __CIL__ in order to set up the reconstruction problem and then solve it. In order to be able to reconstruct both $u$ and $v$ at the same time, we will make use of `BlockDataContainer`. In the following we will define an operator which allows us to access these two items and then we define a function to calculate the joint TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMap(LinearOperator):\n",
    "    \n",
    "    def __init__(self, domain_geometry, index, range_geometry=None):\n",
    "        \n",
    "        self.index = index\n",
    "        if range_geometry is None:\n",
    "            range_geometry = domain_geometry.geometries[self.index]\n",
    "            \n",
    "        super(ProjectionMap, self).__init__(domain_geometry=domain_geometry, \n",
    "                                           range_geometry=range_geometry)   \n",
    "        \n",
    "    def direct(self,x,out=None):\n",
    "                        \n",
    "        if out is None:\n",
    "            return x[self.index]\n",
    "        else:\n",
    "            out.fill(x[self.index])\n",
    "    \n",
    "    def adjoint(self,x, out=None):\n",
    "        \n",
    "        if out is None:\n",
    "            tmp = self.domain_geometry().allocate()\n",
    "            tmp[self.index].fill(x)            \n",
    "            return tmp\n",
    "        else:\n",
    "            out[self.index].fill(x) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothJointTV(Function):\n",
    "              \n",
    "    def __init__(self, epsilon, axis, lambda_par):\n",
    "                \n",
    "        r'''\n",
    "        :param epsilon: smoothing parameter making MixedL21Norm differentiable \n",
    "        '''\n",
    "\n",
    "        #TODO L=??\n",
    "        super(SmoothJointTV, self).__init__(L=numpy.sqrt(8))\n",
    "        \n",
    "        # smoothing parameter\n",
    "        self.epsilon = epsilon   \n",
    "        \n",
    "        # GradientOperator\n",
    "        ig = ImageGeometry(voxel_num_z = 1,voxel_num_y = 3, voxel_num_x = 4)\n",
    "        FDy = FiniteDifferenceOperator(u_simple, direction=1)\n",
    "        FDx = FiniteDifferenceOperator(u_simple, direction=2)\n",
    "        self.grad = BlockOperator(FDy, FDx)\n",
    "        \n",
    "        \n",
    "        # Which variable to differentiate\n",
    "        self.axis = axis\n",
    "        \n",
    "        if self.epsilon==0:\n",
    "            raise ValueError('Working with smooth JTV atm')\n",
    "            \n",
    "        self.lambda_par=lambda_par    \n",
    "                                    \n",
    "                            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        r\"\"\" x is BlockDataContainer that contains (u,v). Actually x is a BlockDataContainer that contains 2 BDC.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, BlockDataContainer):\n",
    "            raise ValueError('__call__ expected BlockDataContainer, got {}'.format(type(x))) \n",
    "\n",
    "        tmp = numpy.abs((self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.epsilon**2).sqrt().sum())\n",
    "        #print('JTV', tmp)\n",
    "        return tmp    \n",
    "                        \n",
    "             \n",
    "    def gradient(self, x, out=None):\n",
    "        \n",
    "        denom = (self.lambda_par*self.grad.direct(x[0]).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x[1]).pnorm(2).power(2)+\\\n",
    "              self.epsilon**2).sqrt()         \n",
    "        \n",
    "        if self.axis==0:            \n",
    "            num = self.lambda_par*self.grad.direct(x[0])                        \n",
    "        else:            \n",
    "            num = (1-self.lambda_par)*self.grad.direct(x[1])            \n",
    "\n",
    "        if out is None:    \n",
    "            tmp = self.grad.range.allocate()\n",
    "            tmp[self.axis].fill(self.grad.adjoint(num.divide(denom)))\n",
    "            return tmp\n",
    "        else:                                \n",
    "            self.grad.adjoint(num.divide(denom), out=out[self.axis])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put everything together and define our two objective functions which solve the two subproblems which we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 0.02\n",
    "alpha2 = 0.02\n",
    "lambda_par = 0.5\n",
    "epsilon = 1e-12\n",
    "\n",
    "bg = BlockGeometry(u_simple, v_simple)\n",
    "\n",
    "L1 = ProjectionMap(bg, index=0)\n",
    "L2 = ProjectionMap(bg, index=1)\n",
    "\n",
    "f1 = 0.5*L2NormSquared(b=g1)\n",
    "f2 = 0.5*L2NormSquared(b=g2)\n",
    "\n",
    "JTV1 = alpha1*SmoothJointTV(epsilon=epsilon, axis=0, lambda_par = lambda_par )\n",
    "JTV2 = alpha2*SmoothJointTV(epsilon=epsilon, axis=1, lambda_par = 1-lambda_par)\n",
    "objective1 = OperatorCompositionFunction(f1, CompositionOperator(A1, L1)) + JTV1\n",
    "objective2 = OperatorCompositionFunction(f2, CompositionOperator(A2, L2)) + JTV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with zero-filled images\n",
    "x0 = bg.allocate(0.0)\n",
    "\n",
    "# We use a fixed step-size for the gradient descent approach\n",
    "step_size = 0.1\n",
    "\n",
    "# We are also going to log the value of the objective functions\n",
    "obj1_val_it = []\n",
    "obj2_val_it = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    gd1 = GD(x0, objective1, step_size=step_size, \\\n",
    "          max_iteration = 4, update_objective_interval = 1)\n",
    "    gd1.run(verbose=1)\n",
    "    \n",
    "    # We skip the first one because it gets repeated\n",
    "    obj1_val_it.extend(gd1.objective[1:])\n",
    "    \n",
    "    # Here we are going to do a little \"trick\" in order to better see, when each subproblem is optimised, we\n",
    "    # are going to append NaNs to the objective function which is currently not optimised. The NaNs will not\n",
    "    # show up in the final plot and hence we can nicely see each subproblem.\n",
    "    obj2_val_it.extend(numpy.ones_like(gd1.objective[1:])*numpy.nan)\n",
    "    \n",
    "    gd2 = GD(gd1.solution, objective2, step_size=step_size, \\\n",
    "          max_iteration = 4, update_objective_interval = 1)\n",
    "    gd2.run(verbose=1)\n",
    "    \n",
    "    obj2_val_it.extend(gd2.objective[1:])\n",
    "    obj1_val_it.extend(numpy.ones_like(gd2.objective[1:])*numpy.nan)\n",
    "    \n",
    "    x0.fill(gd2.solution)\n",
    "    \n",
    "    print('* * * * * * Outer Iteration ', i, ' * * * * * *\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at the images $u_{jtv}$ and $v_{jtv}$ and compare them to the simple reconstruction $u_{simple}$ and $v_{simple}$ and the original ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_jtv = numpy.squeeze(numpy.abs(x0[0].as_array()))\n",
    "v_jtv = numpy.squeeze(numpy.abs(x0[1].as_array()))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([2,3,1], numpy.squeeze(numpy.abs(u_simple.as_array()[0, 70:180, 70:180])), '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,2], u_jtv[70:180, 70:180], '$u_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,3], numpy.squeeze(numpy.abs(u_gt.as_array()[0, 70:180, 70:180])), '$u_{gt}$', cmap=\"Greys_r\") \n",
    "\n",
    "plot_2d_image([2,3,4], numpy.squeeze(numpy.abs(v_simple.as_array()[0, 70:180, 70:180])), '$v_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,5], v_jtv[70:180, 70:180], '$v_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,6], numpy.squeeze(numpy.abs(v_gt.as_array()[0, 70:180, 70:180])), '$v_{gt}$', cmap=\"Greys_r\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(obj1_val_it, 'o-', label='subproblem 1')\n",
    "plt.plot(obj2_val_it, '+-', label='subproblem 2')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Value of objective function')\n",
    "plt.title('Objective functions')\n",
    "plt.legend()\n",
    "\n",
    "# Logarithmic y-axis\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a good demonstration for a synergistic image reconstruction of two different images. The following gives a few suggestions of what to do next and also how to extend this notebook to other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of iterations\n",
    "In our problem we have several regularisation parameters such as $\\alpha_{1}$, $\\alpha_{2}$ and $\\lambda$. In addition, the number of inner iterations for each subproblem (currently set to 3) and the number of outer iterations (currently set to 10) also determine the final solution. Of course, for infinite number of total iterations it shouldn't matter but usually we don't have that much time.\n",
    "\n",
    "__TODO__: Change the number of iterations and see what happens to the objective functions. For a given number of total iterations, do you think it is better to have a high number of inner or high number of outer iterations? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial misalignment\n",
    "In the above example we simulated our data such that there is a perfect spatial match between $u$ and $v$. For real world applications this usually cannot be assumed. \n",
    "\n",
    "__TODO__: Add spatial misalignment between $u$ and $v$. This can be achieved e.g. by calling `numpy.roll` on `T2_arr` before calling `v_gt = crop_and_fill(im_mr, T2_arr)`. What is the effect on the reconstructed images? For a more \"advanced\" misalignment, have a look at notebook `BrainWeb`.\n",
    "\n",
    "__TODO__: One way to minimize spatial misalignment is to use image registration to ensure both $u$ and $v$ are well aligned. In the notebook `sirf_registration` you find information about how to register two images and also how to resample one image based on the spatial transformation estimated from the registration. Try to use this to correct for the misalignment you introduced above. For a real world example, at which point in the code would you have to carry out the registration+resampling? (some more information can also be found at the end of notebook `de_Pierro_MAPEM`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathologies\n",
    "The images $u$ and $v$ show the same anatomy, just with a different contrast. Clinically more useful are of course images which show complementary image information.\n",
    "\n",
    "__TODO__: Add a pathology to either $u$ and $v$ and see how this effects the reconstruction. For something more advanced, have a loot at the notebook `BrainWeb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single anatomical prior\n",
    "So far we have alternated between two reconstruction problems. Another option is to do a single regularised reconstruction and simply use a previously reconstructed image for regularisation.\n",
    "\n",
    "__TODO__: Adapt the above code such that $u$ is reconstructed first without regularisation and is then used for a regularised reconstruction of $v$ without any further updates of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementary k-space trajectories\n",
    "We used the same k-space trajectory for $u$ and $v$. This is of course not ideal for such an optimisation, because the same k-space trajectory also means the same pattern of undersampling artefacts. Of course the artefacts in each image will be different because of the different image content but it still would be better if $u$ and $v$ were acquired with different k-space trajectories.\n",
    "\n",
    "__TODO__: Create two different k-space trajectories and compare the results to a reconstruction using the same k-space trajectories. In order to create a new k-space trajectory, we can take a subset of the available `AcquisitionData`. Here is a short tutorial:\n",
    "\n",
    "After `preprocessed_data = mr.preprocess_acquisition_data(mr_acq)` we can find out which k-space points have been acquired in this acquisition using:\n",
    "\n",
    "```encode_step_1 = preprocessed_data.parameter_info('kspace_encode_step_1')```\n",
    "\n",
    "then we can define an index list of k-space points from `encode_step_1` which we want to use:\n",
    "\n",
    "```acq_new_idx = numpy.arange(0, len(encode_step_1))```\n",
    "\n",
    "This is of course boring, because we are using all k-space points but I am sure you will come up with something more clever. Now we create an empty `AcquisitionData` object and then add the `Acquisitions` which we have selected:\n",
    "\n",
    "```\n",
    "acq_new = preprocessed_data.new_acquisition_data(empty=True)\n",
    "\n",
    "for jnd in range(len(acq_new_idx)):\n",
    "    cacq = preprocessed_data.acquisition(acq_new_idx[jnd])\n",
    "    acq_new.append_acquisition(cacq)\n",
    "    \n",
    "acq_new.sort() \n",
    "```\n",
    "\n",
    "Now we have got our new `AcquisitionData` we can create a new `AcquisitionModel` from it:\n",
    "\n",
    "```\n",
    "A1_new = mr.AcquisitionModel(acq_new, im_mr)\n",
    "A1_new.set_coil_sensitivity_maps(csm)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other regularisation options\n",
    "In this example we used a TV-based regularisation, but of course other regularisers could also be used, such as directional TV.\n",
    "\n",
    "__TODO__: Have a look at the __CIL__ notebook `02_Dynamic_CT` and adapt the `SmoothJointTV` class above to use directional TV. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
