{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint TV for multi-contrast MR\n",
    "This demonstration shows how to do a synergistic reconstruction of two MR images with different contrast. Both MR images show the same underlying anatomy but of course with different contrast. In order to make use of this similarity a joint total variation (TV) operator is used as a regularisation in an iterative image reconstruction approach. \n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "You could export it as a Python file and run it one go, but that might\n",
    "make little sense as the figures are not labelled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Christoph Kolbitsch, Evangelos Papoutsellis, Edoardo Pasca\n",
    "First version: 16th of June 2021  \n",
    "\n",
    "CCP PETMR Synergistic Image Reconstruction Framework (SIRF).  \n",
    "Copyright 2021 Rutherford Appleton Laboratory STFC.    \n",
    "Copyright 2021 Physikalisch-Technische Bundesanstalt.\n",
    "\n",
    "This is software developed for the Collaborative Computational\n",
    "Project in Positron Emission Tomography and Magnetic Resonance imaging\n",
    "(http://www.ccppetmr.ac.uk/).\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything is installed that we need\n",
    "!pip install brainweb nibabel --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports etc\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import brainweb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "# Import MR functionality\n",
    "import sirf.Gadgetron as mr\n",
    "\n",
    "# Import CIL functionality\n",
    "from sirf.Utilities import examples_data_path\n",
    "from cil.framework import  AcquisitionGeometry, BlockDataContainer, BlockGeometry, ImageGeometry\n",
    "from cil.optimisation.functions import Function, OperatorCompositionFunction, SmoothMixedL21Norm, L1Norm, L2NormSquared, BlockFunction, MixedL21Norm, IndicatorBox, TotalVariation, LeastSquares, ZeroFunction\n",
    "from cil.optimisation.operators import GradientOperator, BlockOperator, ZeroOperator, CompositionOperator, LinearOperator, FiniteDifferenceOperator\n",
    "from cil.optimisation.algorithms import PDHG, FISTA, GD\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define some handy function definitions\n",
    "# To make subsequent code cleaner, we have a few functions here. You can ignore\n",
    "# ignore them when you first see this demo.\n",
    "\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def crop_and_fill(templ_im, vol):\n",
    "    \"\"\"Crop volumetric image data and replace image content in template image object\"\"\"\n",
    "    # Get size of template image and crop\n",
    "    idim_orig = templ_im.as_array().shape\n",
    "    idim = (1,)*(3-len(idim_orig)) + idim_orig\n",
    "    offset = (numpy.array(vol.shape) - numpy.array(idim)) // 2\n",
    "    vol = vol[offset[0]:offset[0]+idim[0], offset[1]:offset[1]+idim[1], offset[2]:offset[2]+idim[2]]\n",
    "    \n",
    "    # Make a copy of the template to ensure we do not overwrite it\n",
    "    templ_im_out = templ_im.copy()\n",
    "    \n",
    "    # Fill image content \n",
    "    templ_im_out.fill(numpy.reshape(vol, idim_orig))\n",
    "    return(templ_im_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint TV reconstruction of two MR images\n",
    "\n",
    "Assume we want to reconstruct two MR images $u$ and $v$ and utilse the similarity between both images using a joint TV ($JTV$) operator we can formulate the reconstruction problem as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(u^{*}, v^{*}) \\in \\underset{u,v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} +  \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* $JTV_{\\eta, \\lambda}(u_{1}, u_{2}) = \\sum \\sqrt{ \\lambda|\\nabla u_{1}|^{2} + (1-\\lambda)|\\nabla u_{2}|^{2} + \\eta^{2}}$\n",
    "* $A_{1}$, $A_{2}$: __MR__ `AcquisitionModel`\n",
    "* $g_{1}$, $g_{2}$ __MR__ `AcquisitionData`\n",
    "\n",
    "\n",
    "### Solving this problem \n",
    "\n",
    "In order to solve the above minimization problem, we will use an alternating minimisation approach, where one variable is fixed and we solve wrt to the other variable:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u^{k+1} & = \\underset{u}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{1} u - g\\|^{2}_{2} + \\alpha_{1}\\,\\mathrm{JTV}_{\\eta, \\lambda}(u, v^{k}) \\\\\n",
    "v^{k+1} & = \\underset{v}{\\operatorname{argmin}} \\frac{1}{2} \\| A_{2} v - h\\|^{2}_{2} + \\alpha_{2}\\,\\mathrm{JTV}_{\\eta, 1-\\lambda}(u^{k+1}, v)\n",
    "\\end{align*}$$\n",
    "\n",
    "We are going to use a gradient descent approach to solve each of these subproblems alternatingly.\n",
    "\n",
    "<!-- * The *regularisation parameter* `alpha` should be different for each subproblem. But not to worry at this stage. Maybe we should use $\\alpha_{1}, \\alpha_{2}$ in front of the two JTVs and a $\\lambda$, $1-\\lambda$ for the first JTV and $1-\\lambda$, $\\lambda$, for the second JTV with $0<\\lambda<1$. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on several other notebooks and hence certain steps will be carried out with minimal documentation. If you want more explainations, then we would like to ask you to refer to the corresponding notebooks which are mentioned in the following list. The steps we are going to carry out are\n",
    "\n",
    "  - (A) Get a T1 and T2 map from brainweb which we are going to use as ground truth $u_{gt}$ and $v_{gt}$ for our reconstruction (further information: `introduction` notebook)\n",
    "  \n",
    "  - (B) Create __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ (further information: `acquisition_model_mr_pet_ct` notebook)\n",
    "  \n",
    "  - (C) Set up the joint TV reconstruction problem\n",
    "  \n",
    "  - (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (A) Get brainweb data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and use data from the brainweb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_arr  = vol['T2']\n",
    "T1_arr   = vol['T1']\n",
    "\n",
    "# Normalise image data\n",
    "T2_arr /= numpy.max(T2_arr)\n",
    "T1_arr /= numpy.max(T1_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "slice_show = T2_arr.shape[0]//2\n",
    "plot_2d_image([1,2,1], T2_arr[slice_show, 100:-100, 100:-100], 'T2', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], T1_arr[slice_show, 100:-100, 100:-100], 'T1', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (B) Simulate undersampled MR AcquisitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MR AcquisitionData\n",
    "mr_acq = mr.AcquisitionData(examples_data_path('MR') + '/simulated_MR_2D_cartesian_Grappa2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CSM\n",
    "preprocessed_data = mr.preprocess_acquisition_data(mr_acq)\n",
    "preprocessed_data.sort()\n",
    "\n",
    "csm = mr.CoilSensitivityData()\n",
    "csm.smoothness = 50\n",
    "csm.calculate(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate image template\n",
    "recon = mr.FullySampledReconstructor()\n",
    "recon.set_input(preprocessed_data)\n",
    "recon.process()\n",
    "im_mr = recon.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "csm_arr = numpy.abs(csm.as_array())\n",
    "im_mr_arr = numpy.abs(im_mr.as_array())\n",
    "\n",
    "plot_2d_image([1,3,1], csm_arr[0, 0, :, :], 'Coil 0', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,2], csm_arr[2, 0, :, :], 'Coil 2', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,3,3], im_mr_arr[0, :, :], 'Im', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to create the two __MR__ `AcquisitionModel` $A_{1}$ and $A_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two MR acquisition models\n",
    "A1 = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "A1.set_coil_sensitivity_maps(csm)\n",
    "\n",
    "A2 = mr.AcquisitionModel(preprocessed_data, im_mr)\n",
    "A2.set_coil_sensitivity_maps(csm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and simulate undersampled __MR__ `AcquisitionData` $g_{1}$ and $g_{2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MR\n",
    "u_gt = crop_and_fill(im_mr, T1_arr)\n",
    "g1 = A1.forward(u_gt)\n",
    "\n",
    "v_gt = crop_and_fill(im_mr, T2_arr)\n",
    "g2 = A2.forward(v_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check we are going to apply the backward/adjoint operation to do a simply image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple reconstruction\n",
    "u_simple = A1.backward(g1)\n",
    "v_simple = A2.backward(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display it\n",
    "plt.figure();\n",
    "plot_2d_image([1,2,1], numpy.abs(u_simple.as_array())[0, 70:180, 70:180], '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([1,2,2], numpy.abs(v_simple.as_array())[0, 70:180, 70:180], '$v_{simple}$', cmap=\"Greys_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C) Set up the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used mainly __SIRF__ functionality, now we are going to use __CIL__ in order to set up the reconstruction problem and then solve it. In order to be able to reconstruct both $u$ and $v$ at the same time, we will make use of `BlockDataContainer`. In the following we will define an operator which allows us to access these two items and then we define a function to calculate the joint TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMap(LinearOperator):\n",
    "    \n",
    "    def __init__(self, domain_geometry, index, range_geometry=None):\n",
    "        \n",
    "        self.index = index\n",
    "        if range_geometry is None:\n",
    "            range_geometry = domain_geometry.geometries[self.index]\n",
    "            \n",
    "        super(ProjectionMap, self).__init__(domain_geometry=domain_geometry, \n",
    "                                           range_geometry=range_geometry)   \n",
    "        \n",
    "    def direct(self,x,out=None):\n",
    "                        \n",
    "        if out is None:\n",
    "            return x.get_item(self.index)\n",
    "        else:\n",
    "            out.fill(x.get_item(self.index))\n",
    "    \n",
    "    def adjoint(self,x, out=None):\n",
    "        \n",
    "        if out is None:\n",
    "            tmp = self.domain_geometry().allocate()\n",
    "            tmp[self.index].fill(x)            \n",
    "            return tmp\n",
    "        else:\n",
    "            out[self.index].fill(x) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothJointTV(Function):\n",
    "              \n",
    "    def __init__(self, epsilon, axis, lambda_par):\n",
    "                \n",
    "        r'''\n",
    "        :param epsilon: smoothing parameter making MixedL21Norm differentiable \n",
    "        '''\n",
    "\n",
    "        #TODO L=??\n",
    "        super(SmoothJointTV, self).__init__(L=numpy.sqrt(8))\n",
    "        \n",
    "        # smoothing parameter\n",
    "        self.epsilon = epsilon   \n",
    "        \n",
    "        # GradientOperator\n",
    "        ig = ImageGeometry(voxel_num_z = 1,voxel_num_y = 3, voxel_num_x = 4)\n",
    "        FDy = FiniteDifferenceOperator(u_simple, direction=1)\n",
    "        FDx = FiniteDifferenceOperator(u_simple, direction=2)\n",
    "        self.grad = BlockOperator(FDy, FDx)\n",
    "        \n",
    "        \n",
    "        # Which variable to differentiate\n",
    "        self.axis = axis\n",
    "        \n",
    "        if self.epsilon==0:\n",
    "            raise ValueError('Working with smooth JTV atm')\n",
    "            \n",
    "        self.lambda_par=lambda_par    \n",
    "                                    \n",
    "                            \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        r\"\"\" x is BlockDataContainer that contains (u,v). Actually x is a BlockDataContainer that contains 2 BDC.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, BlockDataContainer):\n",
    "            raise ValueError('__call__ expected BlockDataContainer, got {}'.format(type(x))) \n",
    "\n",
    "        tmp = numpy.abs((self.lambda_par*self.grad.direct(x.get_item(0)).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x.get_item(1)).pnorm(2).power(2)+\\\n",
    "              self.epsilon**2).sqrt().sum())\n",
    "        #print('JTV', tmp)\n",
    "        return tmp    \n",
    "                        \n",
    "             \n",
    "    def gradient(self, x, out=None):\n",
    "        \n",
    "        denom = (self.lambda_par*self.grad.direct(x.get_item(0)).pnorm(2).power(2) + (1-self.lambda_par)*self.grad.direct(x.get_item(1)).pnorm(2).power(2)+\\\n",
    "              self.epsilon**2).sqrt()         \n",
    "        \n",
    "        if self.axis==0:            \n",
    "            num = self.lambda_par*self.grad.direct(x.get_item(0))                        \n",
    "        else:            \n",
    "            num = (1-self.lambda_par)*self.grad.direct(x.get_item(1))            \n",
    "\n",
    "        if out is None:    \n",
    "            tmp = self.grad.range.allocate()\n",
    "            tmp[self.axis].fill(self.grad.adjoint(num.divide(denom)))\n",
    "            return tmp\n",
    "        else:                                \n",
    "            self.grad.adjoint(num.divide(denom), out=out[self.axis])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put everything together and define our two objective functions which solve the two subproblems which we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = 0.05\n",
    "alpha2 = 0.05\n",
    "lambda_par = 0.5\n",
    "epsilon = 1e-12\n",
    "\n",
    "bg = BlockGeometry(u_simple, v_simple)\n",
    "\n",
    "L1 = ProjectionMap(bg, index=0)\n",
    "L2 = ProjectionMap(bg, index=1)\n",
    "\n",
    "f1 = 0.5*L2NormSquared(b=g1)\n",
    "f2 = 0.5*L2NormSquared(b=g2)\n",
    "\n",
    "JTV1 = alpha1*SmoothJointTV(epsilon=epsilon, axis=0, lambda_par = lambda_par )\n",
    "JTV2 = alpha2*SmoothJointTV(epsilon=epsilon, axis=1, lambda_par = 1-lambda_par)\n",
    "objective1 = OperatorCompositionFunction(f1, CompositionOperator(A1, L1)) + JTV1\n",
    "objective2 = OperatorCompositionFunction(f2, CompositionOperator(A2, L2)) + JTV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (D) Solve the joint TV reconstruction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with zero-filled images\n",
    "x0 = bg.allocate(0.0)\n",
    "\n",
    "# We use a fixed step-size for the gradient descent approach\n",
    "step_size = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    gd1 = GD(x0, objective1, step_size=step_size, \\\n",
    "          max_iteration = 3, update_objective_interval = 1)\n",
    "    gd1.run(verbose=1)\n",
    "    \n",
    "    gd2 = GD(gd1.solution, objective2, step_size=step_size, \\\n",
    "          max_iteration = 3, update_objective_interval = 1)\n",
    "    gd2.run(verbose=1)\n",
    "    \n",
    "    x0.fill(gd2.solution)\n",
    "    \n",
    "    print('* * * * * * Outer Iteration ', i, ' * * * * * *\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can look at the images $u_{jtv}$ and $v_{jtv}$ and compare them to the simple reconstruction $u_{simple}$ and $v_{simple}$ and the original ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_jtv = numpy.squeeze(numpy.abs(x0.get_item(0).as_array()))\n",
    "v_jtv = numpy.squeeze(numpy.abs(x0.get_item(1).as_array()))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([2,3,1], numpy.squeeze(numpy.abs(u_simple.as_array()[0, 70:180, 70:180])), '$u_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,2], u_jtv[70:180, 70:180], '$u_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,3], numpy.squeeze(numpy.abs(u_gt.as_array()[0, 70:180, 70:180])), '$u_{gt}$', cmap=\"Greys_r\") \n",
    "\n",
    "plot_2d_image([2,3,4], numpy.squeeze(numpy.abs(v_simple.as_array()[0, 70:180, 70:180])), '$v_{simple}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,5], v_jtv[70:180, 70:180], '$v_{JTV}$', cmap=\"Greys_r\")\n",
    "plot_2d_image([2,3,6], numpy.squeeze(numpy.abs(v_gt.as_array()[0, 70:180, 70:180])), '$v_{gt}$', cmap=\"Greys_r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
